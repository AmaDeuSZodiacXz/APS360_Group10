{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Train DCGAN to synthetics image and apply Augment Data**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nfrom torchvision.transforms.functional import to_pil_image\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset Paths and Classes\ndata_dir = '/kaggle/input/lung-cancer-histopathological-images'\nclass_dirs = ['adenocarcinoma', 'benign', 'squamous_cell_carcinoma']\nnum_classes = len(class_dirs)\n\n# Data Transformations\ndata_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to 224x224\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)  # Normalize images to [-1, 1]\n])\n\naugmentation_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)  # Normalize images to [-1, 1]\n])\n\n# Custom Dataset Class\nclass LungCancerDataset(Dataset):\n    def __init__(self, images_dir, class_dirs, transform=None):\n        self.images = []\n        self.labels = []\n        self.transform = transform\n        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(class_dirs)}\n\n        for class_name in class_dirs:\n            class_path = os.path.join(images_dir, class_name)\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                self.images.append(image_path)\n                self.labels.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert('RGB')\n\n        # Extract simple texture features from grayscale\n        gray_image = np.array(Image.open(image_path).convert('L'))\n        mean = np.mean(gray_image)\n        variance = np.var(gray_image)\n        contrast = np.max(gray_image) - np.min(gray_image)\n        knowledge_features = torch.tensor([mean, variance, contrast], dtype=torch.float32)\n\n        if self.transform:\n            image = self.transform(image)\n\n        label = self.labels[idx]\n        return image, knowledge_features, torch.tensor(label, dtype=torch.long)\n\n# Create Dataset and Split\ndataset = LungCancerDataset(data_dir, class_dirs, transform=data_transforms)\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Conditional GAN for Data Augmentation\nclass ConditionalGenerator(nn.Module):\n    def __init__(self, z_dim=100, class_dim=10, img_channels=3):\n        super(ConditionalGenerator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, class_dim)\n        input_dim = z_dim + class_dim\n        self.gen = nn.Sequential(\n            nn.ConvTranspose2d(input_dim, 1024, kernel_size=4, stride=1, padding=0),  # Output: 4x4\n            nn.BatchNorm2d(1024),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),    # Output: 8x8\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),     # Output: 16x16\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),     # Output: 32x32\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),      # Output: 64x64\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),  # Output: 128x128\n            nn.Tanh(),\n            nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False)    # Resize to 224x224\n        )\n\n    def forward(self, noise, labels):\n        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)  # [batch_size, class_dim, 1, 1]\n        input = torch.cat((noise, label_embedding), dim=1)\n        return self.gen(input)\n\nclass ConditionalDiscriminator(nn.Module):\n    def __init__(self, class_dim=10, img_channels=3):\n        super(ConditionalDiscriminator, self).__init__()\n        self.label_emb = nn.Embedding(num_classes, class_dim)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(img_channels + class_dim, 64, kernel_size=4, stride=2, padding=1),  # Output: 112x112\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Output: 56x56\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Output: 28x28\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Output: 14x14\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1),  # Output: 7x7\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.final_layer = nn.Sequential(\n            nn.Conv2d(512, 1, kernel_size=7, stride=1, padding=0),  # Output: 1x1\n            nn.Sigmoid()\n        )\n\n    def forward(self, img, labels):\n        label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n        label_embedding = label_embedding.expand(labels.size(0), -1, img.size(2), img.size(3))\n        input = torch.cat((img, label_embedding), dim=1)\n        x = self.conv_layers(input)\n        x = self.final_layer(x)\n        return x.view(-1, 1)\n\n# Synthetic Data Generation (Conditional GAN)\ndef train_cgan(train_loader, z_dim=100, class_dim=10, epochs=50):\n    generator = ConditionalGenerator(z_dim=z_dim, class_dim=class_dim).to(device)\n    discriminator = ConditionalDiscriminator(class_dim=class_dim).to(device)\n\n    criterion = nn.BCELoss()\n    optim_gen = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    optim_disc = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    for epoch in range(epochs):\n        epoch_loss_d = 0.0\n        epoch_loss_g = 0.0\n        num_batches = len(train_loader)\n        with tqdm(total=num_batches, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n            for real_imgs, _, labels in train_loader:\n                real_imgs = real_imgs.to(device)\n                labels = labels.to(device)\n                batch_size = real_imgs.size(0)\n\n                # ---------------------\n                # Train Discriminator\n                # ---------------------\n                discriminator.zero_grad()\n\n                # Train on real images\n                real_output = discriminator(real_imgs, labels)\n                real_labels = torch.ones_like(real_output, device=device)\n                real_loss = criterion(real_output, real_labels)\n\n                # Train on fake images\n                noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n                fake_imgs = generator(noise, labels)\n                fake_output = discriminator(fake_imgs.detach(), labels)\n                fake_labels = torch.zeros_like(fake_output, device=device)\n                fake_loss = criterion(fake_output, fake_labels)\n\n                # Total discriminator loss\n                disc_loss = real_loss + fake_loss\n                disc_loss.backward()\n                optim_disc.step()\n\n                # -----------------\n                # Train Generator\n                # -----------------\n                generator.zero_grad()\n\n                noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n                fake_imgs = generator(noise, labels)\n                output = discriminator(fake_imgs, labels)\n                gen_labels = torch.ones_like(output, device=device)  # Generator tries to make discriminator output ones\n                gen_loss = criterion(output, gen_labels)\n                gen_loss.backward()\n                optim_gen.step()\n\n                # Update epoch losses\n                epoch_loss_d += disc_loss.item()\n                epoch_loss_g += gen_loss.item()\n\n                pbar.set_postfix({\"D Loss\": f\"{disc_loss.item():.4f}\", \"G Loss\": f\"{gen_loss.item():.4f}\"})\n                pbar.update(1)\n\n        avg_loss_d = epoch_loss_d / num_batches\n        avg_loss_g = epoch_loss_g / num_batches\n        print(f\"Epoch [{epoch + 1}/{epochs}] Completed | Avg D Loss: {avg_loss_d:.4f} | Avg G Loss: {avg_loss_g:.4f}\")\n\n    return generator\n\n# Generate Synthetic Data for Each Class\ndef generate_synthetic_images(generator, num_images_per_class=2000):\n    generator.eval()\n    synthetic_images = {i: [] for i in range(num_classes)}\n    with torch.no_grad():\n        for class_idx in range(num_classes):\n            num_generated = 0\n            while num_generated < num_images_per_class:\n                noise = torch.randn(1, 100, 1, 1, device=device)\n                labels = torch.tensor([class_idx], device=device)\n                img = generator(noise, labels).squeeze(0).cpu()\n                synthetic_images[class_idx].append(img)\n                num_generated += 1\n    return synthetic_images\n\n# Train the Conditional GAN\ngenerator = train_cgan(train_loader)\n\n# Generate synthetic images for each class\nsynthetic_images = generate_synthetic_images(generator, num_images_per_class=2000)\n\n# Save synthetic images for visualization (optional)\nos.makedirs(\"synthetic_data\", exist_ok=True)\nfor class_idx, images in synthetic_images.items():\n    class_dir = f\"synthetic_data/class_{class_idx}\"\n    os.makedirs(class_dir, exist_ok=True)\n    for idx, img_tensor in enumerate(images):\n        # Normalize the tensor from [-1, 1] to [0, 1]\n        img_tensor = (img_tensor + 1) / 2\n        # Ensure the tensor is in the correct data type\n        img_tensor = img_tensor.clamp(0, 1)\n        # Save the image using save_image\n        save_image(img_tensor, f\"{class_dir}/synthetic_img_{idx}.png\")\n\n# Add synthetic images to dataset\nclass SyntheticDataset(Dataset):\n    def __init__(self, synthetic_images, class_idx, transform=None):\n        self.images = synthetic_images\n        self.labels = [class_idx] * len(synthetic_images)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_tensor = self.images[idx]\n        label = self.labels[idx]\n        \n        # Normalize the tensor from [-1, 1] to [0, 1]\n        img_tensor = (img_tensor + 1) / 2\n        img_tensor = img_tensor.clamp(0, 1)\n\n        # Apply transforms\n        if self.transform:\n            # Convert the tensor to PIL Image\n            img = to_pil_image(img_tensor)\n            img = self.transform(img)\n        else:\n            # If no transform is provided, convert to tensor and normalize\n            img = img_tensor\n            img = transforms.Normalize([0.5]*3, [0.5]*3)(img)\n        \n        # Extract synthetic texture features\n        img_gray = img_tensor.mean(dim=0).numpy() * 255  # Convert to grayscale\n        mean = np.mean(img_gray)\n        variance = np.var(img_gray)\n        contrast = np.max(img_gray) - np.min(img_gray)\n        knowledge_features = torch.tensor([mean, variance, contrast], dtype=torch.float32)\n\n        return img, knowledge_features, torch.tensor(label, dtype=torch.long)\n\n# Create synthetic datasets for all classes\nsynthetic_datasets = []\nfor class_idx in range(num_classes):\n    synthetic_class_images = synthetic_images[class_idx]\n    synthetic_dataset = SyntheticDataset(synthetic_class_images, class_idx, transform=data_transforms)\n    synthetic_datasets.append(synthetic_dataset)\n\n# Combine synthetic and original datasets\ncombined_train_dataset = ConcatDataset([train_dataset] + synthetic_datasets)\n\n# Create dataloaders for combined dataset\ncombined_train_loader = DataLoader(combined_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T03:05:07.183315Z","iopub.execute_input":"2024-11-28T03:05:07.183650Z","iopub.status.idle":"2024-11-28T05:31:20.139504Z","shell.execute_reply.started":"2024-11-28T03:05:07.183610Z","shell.execute_reply":"2024-11-28T05:31:20.138399Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 329/329 [02:42<00:00,  2.03batch/s, D Loss=0.2110, G Loss=8.5849]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50] Completed | Avg D Loss: 0.5025 | Avg G Loss: 5.1297\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 329/329 [02:51<00:00,  1.92batch/s, D Loss=2.8644, G Loss=3.5511]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/50] Completed | Avg D Loss: 1.0558 | Avg G Loss: 2.9315\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.3983, G Loss=1.1131]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/50] Completed | Avg D Loss: 1.0366 | Avg G Loss: 2.8322\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 329/329 [02:51<00:00,  1.91batch/s, D Loss=0.7420, G Loss=2.5934]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/50] Completed | Avg D Loss: 0.9791 | Avg G Loss: 3.3280\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=1.6254, G Loss=3.4245] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/50] Completed | Avg D Loss: 0.8549 | Avg G Loss: 3.4964\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 329/329 [02:51<00:00,  1.91batch/s, D Loss=0.5910, G Loss=3.3663]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/50] Completed | Avg D Loss: 0.8554 | Avg G Loss: 3.1155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.4104, G Loss=3.6680]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/50] Completed | Avg D Loss: 0.7731 | Avg G Loss: 3.5018\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0618, G Loss=3.8445]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/50] Completed | Avg D Loss: 0.7249 | Avg G Loss: 3.5180\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=3.2632, G Loss=0.5475] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/50] Completed | Avg D Loss: 0.8611 | Avg G Loss: 3.5031\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.1361, G Loss=5.2629]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/50] Completed | Avg D Loss: 0.9134 | Avg G Loss: 3.2467\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=4.7535, G Loss=3.7688]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/50] Completed | Avg D Loss: 0.9816 | Avg G Loss: 3.1936\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.4485, G Loss=2.5900]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/50] Completed | Avg D Loss: 0.8919 | Avg G Loss: 3.2181\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=1.1287, G Loss=2.8027] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/50] Completed | Avg D Loss: 0.8172 | Avg G Loss: 3.4642\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.5166, G Loss=1.4169]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/50] Completed | Avg D Loss: 0.7584 | Avg G Loss: 3.7692\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.6078, G Loss=7.7340] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/50] Completed | Avg D Loss: 0.6990 | Avg G Loss: 4.0194\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.7049, G Loss=4.6188] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/50] Completed | Avg D Loss: 0.6406 | Avg G Loss: 4.2278\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=1.4231, G Loss=0.0289] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/50] Completed | Avg D Loss: 0.6310 | Avg G Loss: 4.3728\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.1462, G Loss=4.3895] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/50] Completed | Avg D Loss: 0.5622 | Avg G Loss: 4.5450\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.3804, G Loss=6.1151] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/50] Completed | Avg D Loss: 0.5822 | Avg G Loss: 4.6954\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.2377, G Loss=11.9331]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/50] Completed | Avg D Loss: 0.5959 | Avg G Loss: 4.9155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.2168, G Loss=5.6992] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/50] Completed | Avg D Loss: 0.5009 | Avg G Loss: 4.8159\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.8257, G Loss=8.0300] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/50] Completed | Avg D Loss: 0.4970 | Avg G Loss: 4.9861\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=1.0031, G Loss=1.4600] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/50] Completed | Avg D Loss: 0.4830 | Avg G Loss: 5.3048\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.7087, G Loss=0.7291] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/50] Completed | Avg D Loss: 0.4732 | Avg G Loss: 5.0818\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=1.5763, G Loss=4.3764] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/50] Completed | Avg D Loss: 0.4661 | Avg G Loss: 5.2467\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.4856, G Loss=8.1413] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [26/50] Completed | Avg D Loss: 0.4373 | Avg G Loss: 5.4212\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.5177, G Loss=0.9602] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [27/50] Completed | Avg D Loss: 0.4471 | Avg G Loss: 5.6954\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0374, G Loss=8.2822] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [28/50] Completed | Avg D Loss: 0.4065 | Avg G Loss: 5.6469\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.3765, G Loss=9.6153] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [29/50] Completed | Avg D Loss: 0.3775 | Avg G Loss: 7.2428\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.7644, G Loss=8.5599] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [30/50] Completed | Avg D Loss: 0.2940 | Avg G Loss: 7.6886\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.0544, G Loss=8.5583] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [31/50] Completed | Avg D Loss: 0.2467 | Avg G Loss: 8.9862\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0051, G Loss=18.0142]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [32/50] Completed | Avg D Loss: 0.1698 | Avg G Loss: 11.0716\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0007, G Loss=18.0213]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [33/50] Completed | Avg D Loss: 0.1604 | Avg G Loss: 10.2260\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.0171, G Loss=8.5385] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [34/50] Completed | Avg D Loss: 0.1806 | Avg G Loss: 11.5831\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.1139, G Loss=10.0248]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [35/50] Completed | Avg D Loss: 0.2760 | Avg G Loss: 10.2815\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0421, G Loss=8.4665] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [36/50] Completed | Avg D Loss: 0.2855 | Avg G Loss: 8.7053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.0665, G Loss=20.0449]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [37/50] Completed | Avg D Loss: 0.2515 | Avg G Loss: 8.8040\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0252, G Loss=9.5332] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [38/50] Completed | Avg D Loss: 0.2177 | Avg G Loss: 9.2205\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.0014, G Loss=8.3938] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [39/50] Completed | Avg D Loss: 0.2109 | Avg G Loss: 9.5496\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.0037, G Loss=13.7168]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [40/50] Completed | Avg D Loss: 0.2180 | Avg G Loss: 9.6616\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0079, G Loss=13.1806]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [41/50] Completed | Avg D Loss: 0.1958 | Avg G Loss: 9.9758\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.1566, G Loss=10.7803]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [42/50] Completed | Avg D Loss: 0.2713 | Avg G Loss: 9.2387\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0247, G Loss=9.1894] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [43/50] Completed | Avg D Loss: 0.2445 | Avg G Loss: 9.6724\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0081, G Loss=7.3203] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [44/50] Completed | Avg D Loss: 0.2424 | Avg G Loss: 10.1731\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0712, G Loss=6.9129] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [45/50] Completed | Avg D Loss: 0.2481 | Avg G Loss: 11.2177\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 329/329 [02:52<00:00,  1.90batch/s, D Loss=0.0066, G Loss=18.8319]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [46/50] Completed | Avg D Loss: 0.2525 | Avg G Loss: 10.9915\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 329/329 [02:51<00:00,  1.91batch/s, D Loss=0.1944, G Loss=13.1017]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [47/50] Completed | Avg D Loss: 0.2256 | Avg G Loss: 10.4835\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.1418, G Loss=14.7573]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [48/50] Completed | Avg D Loss: 0.1939 | Avg G Loss: 10.6553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 329/329 [02:52<00:00,  1.91batch/s, D Loss=0.0087, G Loss=10.4737]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [49/50] Completed | Avg D Loss: 0.2839 | Avg G Loss: 8.7344\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 329/329 [02:53<00:00,  1.90batch/s, D Loss=0.1548, G Loss=3.6618] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [50/50] Completed | Avg D Loss: 0.2356 | Avg G Loss: 9.7341\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Train KATANA**","metadata":{}},{"cell_type":"code","source":"# Define the KATANA Model\nclass KATANA(nn.Module):\n    def __init__(self):\n        super(KATANA, self).__init__()\n        \n        # CNN Pathway\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Output: [batch, 32, 224, 224]\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Output: [batch, 64, 112, 112]\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Output: [batch, 128, 56, 56]\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.flatten = nn.Flatten()\n\n        self.cnn_fc1 = nn.Linear(128 * 28 * 28, 256)\n        self.cnn_fc2 = nn.Linear(256, 128)\n        \n        # KAN Pathway\n        self.kan_fc1 = nn.Linear(3, 64)\n        self.kan_fc2 = nn.Linear(64, 32)\n\n        # Fusion Layer\n        self.fc1 = nn.Linear(128 + 32, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x, knowledge_features):\n        # CNN Pathway\n        x = self.pool(self.relu(self.conv1(x)))   # [batch, 32, 112, 112]\n        x = self.pool(self.relu(self.conv2(x)))   # [batch, 64, 56, 56]\n        x = self.pool(self.relu(self.conv3(x)))   # [batch, 128, 28, 28]\n        x = self.flatten(x)\n        x = self.relu(self.cnn_fc1(x))\n        x = self.relu(self.cnn_fc2(x))\n        \n        # KAN Pathway\n        k = self.relu(self.kan_fc1(knowledge_features))\n        k = self.relu(self.kan_fc2(k))\n        \n        # Fusion Layer\n        combined = torch.cat((x, k), dim=1)\n        combined = self.relu(self.fc1(combined))\n        combined = self.dropout(self.relu(self.fc2(combined)))\n        output = self.fc3(combined)\n        \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:35:04.713115Z","iopub.execute_input":"2024-11-28T05:35:04.713737Z","iopub.status.idle":"2024-11-28T05:35:04.723926Z","shell.execute_reply.started":"2024-11-28T05:35:04.713703Z","shell.execute_reply":"2024-11-28T05:35:04.722838Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Initialize the KATANA model\nmodel = KATANA().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training Function with tqdm Progress Bar\ndef train_katana(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        all_preds = []\n        all_labels = []\n        \n        num_batches = len(train_loader)\n        # Training Loop with tqdm progress bar\n        with tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n            for images, knowledge_features, labels in train_loader:\n                images, knowledge_features, labels = images.to(device), knowledge_features.to(device), labels.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(images, knowledge_features)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n                running_loss += loss.item() * images.size(0)\n                _, preds = torch.max(outputs, 1)\n                correct += torch.sum(preds == labels)\n                total += labels.size(0)\n\n                # Collect predictions and labels for F1 score\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n                pbar.update(1)\n                pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n\n        # Calculate training metrics\n        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n        train_loss = running_loss / len(train_loader.dataset)\n        train_acc = correct.double() / total\n        print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n\n        # Validation Loop with tqdm progress bar\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        val_preds = []\n        val_labels = []\n\n        num_val_batches = len(val_loader)\n        with tqdm(total=num_val_batches, desc=\"Validating\", unit=\"batch\", leave=False) as pbar_val:\n            with torch.no_grad():\n                for images, knowledge_features, labels in val_loader:\n                    images, knowledge_features, labels = images.to(device), knowledge_features.to(device), labels.to(device)\n                    outputs = model(images, knowledge_features)\n                    _, predicted = torch.max(outputs, 1)\n                    val_total += labels.size(0)\n                    val_correct += (predicted == labels).sum().item()\n\n                    # Collect predictions and labels for F1 score\n                    val_preds.extend(predicted.cpu().numpy())\n                    val_labels.extend(labels.cpu().numpy())\n\n                    pbar_val.update(1)\n\n        # Calculate validation metrics\n        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n        val_acc = val_correct / val_total\n        print(f\"Epoch {epoch + 1}/{num_epochs}: Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n\n# Train KATANA on combined dataset\ntrain_katana(model, combined_train_loader, val_loader, criterion, optimizer, num_epochs=10)# You can adjust epoch (Bas)\n\n# Save the final trained model\ntorch.save(model.state_dict(), \"katana_final_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:35:15.763830Z","iopub.execute_input":"2024-11-28T05:35:15.764196Z","iopub.status.idle":"2024-11-28T05:51:44.652157Z","shell.execute_reply.started":"2024-11-28T05:35:15.764147Z","shell.execute_reply":"2024-11-28T05:51:44.651362Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 516/516 [01:24<00:00,  6.10batch/s, Loss=0.1606]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10: Train Loss: 0.3640, Train Acc: 0.8884, Train F1: 0.8883\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10: Val Acc: 0.9084, Val F1: 0.9079\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 516/516 [01:23<00:00,  6.19batch/s, Loss=0.0862]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10: Train Loss: 0.1495, Train Acc: 0.9443, Train F1: 0.9442\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10: Val Acc: 0.8964, Val F1: 0.8953\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 516/516 [01:22<00:00,  6.23batch/s, Loss=0.3352]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10: Train Loss: 0.1289, Train Acc: 0.9542, Train F1: 0.9542\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10: Val Acc: 0.9360, Val F1: 0.9357\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 516/516 [01:22<00:00,  6.27batch/s, Loss=0.0027]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10: Train Loss: 0.0878, Train Acc: 0.9671, Train F1: 0.9671\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10: Val Acc: 0.9453, Val F1: 0.9452\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 516/516 [01:22<00:00,  6.25batch/s, Loss=0.0188]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10: Train Loss: 0.0692, Train Acc: 0.9775, Train F1: 0.9775\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10: Val Acc: 0.9449, Val F1: 0.9447\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 516/516 [01:22<00:00,  6.24batch/s, Loss=0.0000]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10: Train Loss: 0.0577, Train Acc: 0.9799, Train F1: 0.9799\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10: Val Acc: 0.9449, Val F1: 0.9445\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 516/516 [01:22<00:00,  6.26batch/s, Loss=0.0000]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10: Train Loss: 0.0273, Train Acc: 0.9912, Train F1: 0.9912\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10: Val Acc: 0.9542, Val F1: 0.9541\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 516/516 [01:22<00:00,  6.23batch/s, Loss=0.0008]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10: Train Loss: 0.0232, Train Acc: 0.9936, Train F1: 0.9936\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10: Val Acc: 0.9600, Val F1: 0.9599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 516/516 [01:22<00:00,  6.23batch/s, Loss=0.0170]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10: Train Loss: 0.0486, Train Acc: 0.9904, Train F1: 0.9904\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10: Val Acc: 0.9413, Val F1: 0.9411\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 516/516 [01:23<00:00,  6.19batch/s, Loss=0.0114]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10: Train Loss: 0.0310, Train Acc: 0.9920, Train F1: 0.9920\n","output_type":"stream"},{"name":"stderr","text":"                                                              \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10: Val Acc: 0.9551, Val F1: 0.9550\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Test KATANA Model**","metadata":{}},{"cell_type":"code","source":"# Testing Function\ndef test_katana(model, test_loader, criterion):\n    model.eval()\n    test_correct = 0\n    test_total = 0\n    test_preds = []\n    test_labels = []\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for images, knowledge_features, labels in tqdm(test_loader, desc=\"Testing\"):\n            images, knowledge_features, labels = images.to(device), knowledge_features.to(device), labels.to(device)\n            outputs = model(images, knowledge_features)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * images.size(0)\n            _, predicted = torch.max(outputs, 1)\n            test_total += labels.size(0)\n            test_correct += (predicted == labels).sum().item()\n\n            # Collect predictions and labels for F1 score\n            test_preds.extend(predicted.cpu().numpy())\n            test_labels.extend(labels.cpu().numpy())\n\n    # Calculate test metrics\n    test_f1 = f1_score(test_labels, test_preds, average='weighted')\n    test_loss /= len(test_loader.dataset)\n    test_acc = test_correct / test_total\n\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test F1 Score: {test_f1:.4f}\")\n\n# Test the final model\ntest_katana(model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T05:51:56.555855Z","iopub.execute_input":"2024-11-28T05:51:56.556230Z","iopub.status.idle":"2024-11-28T05:52:15.615648Z","shell.execute_reply.started":"2024-11-28T05:51:56.556166Z","shell.execute_reply":"2024-11-28T05:52:15.614691Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 71/71 [00:19<00:00,  3.73it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 0.2149, Test Accuracy: 0.9480, Test F1 Score: 0.9480\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}